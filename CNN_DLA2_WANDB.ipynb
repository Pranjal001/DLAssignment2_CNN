{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4502aa019d3c4baf8b3fa152616c251d":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_db8c8df80cb14c11a80cebad8c21248d","IPY_MODEL_787c56a233d041feb960afaea800226a"],"layout":"IPY_MODEL_f511510faf934ea18a010e8186069114"}},"db8c8df80cb14c11a80cebad8c21248d":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd14b4eae8104e72aea1141581169a4e","placeholder":"â€‹","style":"IPY_MODEL_b4d832c35b0b4632b8fb3ec4f7157e43","value":"0.012 MB of 0.012 MB uploaded\r"}},"787c56a233d041feb960afaea800226a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a788cb59d9fd42dfb1c5e7477e74587f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c465fcf99139492a85528626425dcf78","value":1}},"f511510faf934ea18a010e8186069114":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd14b4eae8104e72aea1141581169a4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4d832c35b0b4632b8fb3ec4f7157e43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a788cb59d9fd42dfb1c5e7477e74587f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c465fcf99139492a85528626425dcf78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8030211,"sourceType":"datasetVersion","datasetId":4733049}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb","metadata":{"id":"jpyDr-KJp_tu","outputId":"6bb710fc-2b58-4b1e-e757-f1a20534e985","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n!wandb login '9a5ccd0848378e80e2abf8c49fbe7f9d7c5e0b10'","metadata":{"id":"IfXdtk18qBiR","outputId":"b2034043-b5b6-4a17-b93f-dc201536cc5f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.nn import functional as funct\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nimport os\nfrom PIL import Image\nfrom torch import optim\nimport torch\nfrom torch.nn import Module\nfrom torch.nn import Conv2d\nfrom torch.nn import Linear\nfrom torch.nn import MaxPool2d\nfrom torch.nn import ReLU\nfrom torch.nn import LogSoftmax\nfrom torch import flatten","metadata":{"id":"8BshsEtqc3jH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"t7ngzSyLpT-l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install wget\n# import wget\n# wget.download('https://storage.googleapis.com/wandb_datasets/nature_12K.zip')\n# !unzip /kaggle/working/nature_12K.zip","metadata":{"id":"iGpArvflabm0","outputId":"33c7cffa-9bc9-4d02-a4a7-dde12c3f320d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n\n  def __init__(self,settings):\n    super().__init__()\n    input_channel = settings['input_channel']\n    output_size = settings['output_size']\n    total_filters = settings['total_filters']\n    filter_size = settings['filter_size']\n\n    filter_list_out = self.organise_filter_function(settings['filter_organisation'],total_filters)\n\n    stride_filter = settings['stride']\n    padding_filter = settings['padding']\n    filter_pool = settings['filter_pool']\n    padding_pool = settings['padding_pool']\n    stride_pool = settings['stride_pool']\n    activation = settings['activation']\n\n    activation = self.activation_call(activation)\n\n    dense_layer_size = settings['dense_layer_size']\n    image_size = settings['image_size']\n    dropout_prob = settings['dropout']\n    self.batch_normalisation = settings['batch_normalisation']\n\n# batct normalisation\n    if(self.batch_normalisation == 'Yes'):\n      self.conv1_bn = nn.BatchNorm2d(filter_list_out[0])\n      self.conv2_bn = nn.BatchNorm2d(filter_list_out[1])\n      self.conv3_bn = nn.BatchNorm2d(filter_list_out[2])\n      self.conv4_bn = nn.BatchNorm2d(filter_list_out[3])\n      self.conv5_bn = nn.BatchNorm2d(filter_list_out[4])\n\n# generating convolution layer and pooling layers along with the dense layer with appropriate sizes\n\n    #-------------------------------------------------------------------------------CONV1--------------------------------------------------------------------------------------------\n\n    self.conv1 = nn.Conv2d(in_channels = input_channel, out_channels =filter_list_out[0], kernel_size = filter_size[0], padding = padding_filter, stride = stride_filter)\n    self.activation1 = activation\n    self.pool1 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)\n\n    image_size =  self.compute_conv_size(image_size , filter_size[0] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)\n\n    #--------------------------------------------------------------------------------CONV2---------------------------------------------------------------------------------------\n\n    self.conv2 = nn.Conv2d(in_channels = filter_list_out[0], out_channels =filter_list_out[1], kernel_size = filter_size[1], padding = padding_filter, stride = stride_filter)\n    self.activation2 = activation\n    self.pool2 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)\n\n    image_size =  self.compute_conv_size(image_size , filter_size[1] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)\n\n    #---------------------------------------------------------------------------------CONV3------------------------------------------------------------------------------------------\n\n    self.conv3 = nn.Conv2d(in_channels = filter_list_out[1], out_channels =filter_list_out[2], kernel_size = filter_size[2], padding = padding_filter, stride = stride_filter)\n    self.activation3 = activation\n    self.pool3 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)\n\n    image_size =  self.compute_conv_size(image_size , filter_size[2] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)\n\n    #----------------------------------------------------------------------------------CONV4-----------------------------------------------------------------------------------------\n\n    self.conv4 = nn.Conv2d(in_channels = filter_list_out[2], out_channels =filter_list_out[3], kernel_size = filter_size[3], padding = padding_filter, stride = stride_filter)\n    self.activation4 = activation\n    self.pool4 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)\n\n    image_size =  self.compute_conv_size(image_size , filter_size[3] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)\n\n    #-----------------------------------------------------------------------------------CONV5----------------------------------------------------------------------------------------\n\n    self.conv5 = nn.Conv2d(in_channels = filter_list_out[3], out_channels = filter_list_out[4], kernel_size = filter_size[4], padding = padding_filter, stride = stride_filter)\n    self.activation5 = activation\n    self.pool5 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)\n\n    image_size =  self.compute_conv_size(image_size , filter_size[4] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)\n\n    #-----------------------------------------------------------------------------------DENSE_LAYER-------------------------------------------------------------------------------------\n    self.flatten = nn.Flatten()\n    self.dropout = nn.Dropout(dropout_prob)\n    self.FC1 = nn.Linear(filter_list_out[4]*image_size*image_size,dense_layer_size)\n    self.activationfc1 = activation\n    self.op_layer = nn.Linear(dense_layer_size,output_size)\n\n#=============================================================================== Utility Functions ========================================================================================\n\n  def organise_filter_function(self, org_type, filter_size):\n\n    if(org_type == 'same'):\n      factor = 1\n    if(org_type == 'double'):\n      factor = 2\n    if(org_type == 'half'):\n      factor = 0.5\n\n    filter_list = []\n    for i in range(5):\n      filter_list.append(filter_size)\n      filter_size = int(filter_size*factor)\n\n    return filter_list\n\n  def activation_call(self, act_type):\n\n    if act_type == 'ReLU':\n      return nn.ReLU()\n    if act_type == 'GELU':\n      return nn.GELU()\n    if act_type == 'SiLU':\n      return nn.SiLU()\n    if act_type == 'Mish':\n      return nn.Mish()\n\n  def compute_conv_size(self,W, F, S, P, DP, FP, PP, SP):\n    W =  (W - F + 2*P)//S + 1\n    return (W + 2*PP - (DP* (FP-1)) - 1)//SP + 1\n\n\n\n  def forward(self,data):\n\n    data = self.conv1(data)\n    if(self.batch_normalisation == 'Yes'):\n      data = self.conv1_bn(data)\n    data = self.activation1(data)\n    data = self.pool1(data)\n\n    data = self.conv2(data)\n    if(self.batch_normalisation == 'Yes'):\n      data = self.conv2_bn(data)\n    data = self.activation2(data)\n    data = self.pool2(data)\n\n    data = self.conv3(data)\n    if(self.batch_normalisation == 'Yes'):\n      data = self.conv3_bn(data)\n    data = self.activation3(data)\n    data = self.pool3(data)\n\n    data = self.conv4(data)\n    if(self.batch_normalisation == 'Yes'):\n      data = self.conv4_bn(data)\n    data = self.activation4(data)\n    data = self.pool4(data)\n\n    data = self.conv5(data)\n    if(self.batch_normalisation == 'Yes'):\n      data = self.conv5_bn(data)\n    data = self.activation5(data)\n    data = self.pool5(data)\n\n    data = self.flatten(data)\n    data = self.dropout(data)\n    data = self.FC1(data)\n    data = self.activationfc1(data)\n    data = self.op_layer(data)\n    data = funct.softmax(data,dim = 1)\n\n    return data\n\n\n","metadata":{"id":"-eK_fT1vfr3h","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dataset_loader(image_size , aug_type , batch_size,split_ratio):\n\n  test_trans = transforms.Compose([transforms.Resize((image_size, image_size)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n\n  if(aug_type == 'Yes'):\n\n    train_trans = transforms.Compose([transforms.RandomHorizontalFlip(),transforms.RandomRotation(degrees=30),transforms.Resize((image_size, image_size)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n\n  else:\n    train_trans = test_trans\n\n  dataset = \"/kaggle/input/naturalistdata/inaturalist_12K\"\n  train = datasets.ImageFolder(os.path.join(dataset, 'train'), transform = train_trans)\n  test = datasets.ImageFolder(os.path.join(dataset, 'val'), transform = test_trans)\n\n  class_set = train.classes\n\n  val = 1999\n  tr = 9999-val\n  train,validation = random_split(train, [tr, val])\n\n  load_train = DataLoader(train, batch_size = batch_size, num_workers=4)\n  load_validation = DataLoader(validation, batch_size = batch_size, num_workers=4)\n  load_test = DataLoader(test, batch_size = batch_size, num_workers=4)\n\n\n  return class_set , load_train , load_test ,load_validation\n\n","metadata":{"id":"3REjJ3mCeWnz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sweep_config = {\n    'name' : \"sweep_30_final\",\n    'method': 'random'\n    }\n\nparameters_sweep = {\n\n  'input_channel' : {'values' : [3] },\n  'output_size' : {'values' : [10] },\n  'data_augmentation' : {'values' : ['Yes','No'] }, # \"Yes\", \"No\"\n  'batch_normalisation' : {'values' : ['Yes','No'] }, # \"Yes\", \"No\"\n  'dropout' : {'values' : [0.2,0.1,0.3] },\n  'filter_organisation' : {'values' : ['same','double','half'] }, #'same', 'double', 'half'\n  'total_filters': {'values' : [32,64,128] }, #total filters on each layer\n  'filter_size' : {'values' : [[7,5,5,3,3],[11,7,5,3,3],[3,3,3,3,3]] }, #each filter size\n  'stride' : {'values' : [1,2] }, #stride value for filter\n  'padding' : {'values' : [0] }, #padding value\n  'filter_pool': {'values' : [3,2] }, \n  'stride_pool' : {'values' : [1,2] }, #stride value during pooling\n  'padding_pool' : {'values' : [0] }, #padding value for pooling\n  'activation' : {'values' : ['ReLU', 'GELU', 'SiLU', 'Mish'] }, # ReLU, GELU, SiLU, Mish,\n  'dense_layer_size' : {'values' : [256,512,1024] },\n  'batch_size' : {'values' : [8,32,64] },\n  'image_size' : {'values' : [224,256] },\n  'epochs':{'values' : [5,8,10] },\n  'learning_rate' : {'values' : [0.0001,0.0003] },\n  'optimizer' : {'values' : ['adam','sgd'] },\n  }\n\n\nmetric = {\n    'name' : 'Accuracy',\n    'goal' : 'maximize'\n}\n\nsweep_config['metric'] = metric\n\nsweep_config['parameters'] = parameters_sweep","metadata":{"id":"963zPizCrSNr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model( settings , class_set , load_train , load_test ,load_validation ):\n\n  def assign_optimizer(opt_type , lr , model):\n    if(opt_type  == 'adam'):\n      optimizer = optim.Adam(model.parameters(), lr)\n      return optimizer\n    if(opt_type  == 'sgd'):\n      optimizer = optim.SGD(model.parameters(), lr)\n      return optimizer\n    if(opt_type  == 'nadam'):\n      optimizer = optim.NAdam(model.parameters(), lr)\n      return optimizer\n  \n  model = CNN(settings).to(device)\n  optimizer = assign_optimizer(settings['optimizer'] , settings['learning_rate'] , model)\n  criterion = nn.CrossEntropyLoss()\n\n  for epochit in range(settings['epochs']):\n      temp_loss_train = 0.0\n      pred_train = 0\n      pred_total = 0\n\n\n      model.train()\n\n      for images, labels in load_train:\n\n          images = images.to(device)\n          labels =  labels.to(device)\n\n          optimizer.zero_grad()\n          outputs = model.forward(images)\n          loss = criterion(outputs, labels)\n          loss.backward()\n          optimizer.step()\n\n          temp_loss_train += loss.item()\n          _, predicted = torch.max(outputs.data, 1)\n          pred_total += labels.size(0)\n          pred_train += (predicted == labels).sum().item()\n\n\n      train_accuracy = pred_train / pred_total\n      loss_train = temp_loss_train / len(load_train)\n      print('===================================================================================')\n      print('Epoch ',epochit+1, \"Train Loss:\", loss_train, \"Train Accuracy:\" ,train_accuracy)\n      wandb.log({'epochs':epochit + 1, 'train_loss': loss_train, 'train_accuracy': train_accuracy})\n\n\n\n      model.eval()\n      temp_loss_val = 0.0\n      pred_val = 0\n      pred_total_val = 0\n\n      with torch.no_grad():\n          for val_images, val_labels in load_validation:\n\n              val_images = val_images.to(device)\n              val_labels =  val_labels.to(device)\n              val_outputs = model.forward(val_images)\n              val_loss = criterion(val_outputs, val_labels)\n              temp_loss_val += val_loss.item()\n\n              _, val_predicted = torch.max(val_outputs.data, 1)\n              pred_total_val += val_labels.size(0)\n              pred_val += (val_predicted == val_labels).sum().item()\n\n      val_accuracy = pred_val / pred_total_val\n      val_average_loss = temp_loss_val / len(load_validation)\n      wandb.log({'val_loss': val_average_loss, 'val_accuracy': val_accuracy})\n      print(\"Validation Loss:\", val_average_loss, \"Validation Accuracy:\" ,val_accuracy)\n      print('===================================================================================')\n\n\n  wandb.log({'Accuracy' :val_accuracy})\n\n\n\n","metadata":{"id":"3B6A6I7_nx5C","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n  param = wandb.init(project=\"DL_assignment_2\")\n  wandb.run.name = (\n      \":ep-\" + str(param.config.epochs) +\n      \":opt-\" + param.config.optimizer +\n      \":a-\" + param.config.activation +\n      \":bs-\" + str(param.config.batch_size) +\n      \":fs-\" + str(param.config.filter_size) +\n      \":fp-\" + str(param.config.filter_pool) +\n      \":da-\" + param.config.data_augmentation +\n      \":eta-\" + str(param.config.learning_rate) +\n      \":dls-\" + str( param.config.dense_layer_size)\n  )\n\n  settings = {\n      'input_channel' : param.config.input_channel,\n      'output_size' : param.config.output_size,\n      'data_augmentation' : param.config.data_augmentation, # \"Yes\", \"No\"\n      'batch_normalisation' : param.config.batch_normalisation, # \"Yes\", \"No\"\n      'dropout' : param.config.dropout,\n      'filter_organisation' : param.config.filter_organisation, #'same', 'double', 'half'\n      'total_filters': param.config.total_filters, #total filters on each layer\n      'filter_size' : param.config.filter_size, #each filter size\n      'stride' : param.config.stride, #stride value for filter\n      'padding' : param.config.padding, #padding value\n      'filter_pool': param.config.filter_pool, #pooled filter size(filter_pool_size not needed)\n      'stride_pool' : param.config.stride_pool, #stride value during pooling\n      'padding_pool' : param.config.padding_pool, #padding value for pooling\n      'activation' : param.config.activation, # ReLU, GELU, SiLU, Mish,\n      'dense_layer_size' : param.config.dense_layer_size,\n      'batch_size' : param.config.batch_size,\n      'image_size' : param.config.image_size,\n      'epochs':param.config.epochs,\n      'learning_rate' : param.config.learning_rate,\n      'optimizer' : param.config.optimizer,\n\n  }\n# settings = {\n#   'input_channel' : 3,\n#   'output_size' : 10,\n#   'data_augmentation' : \"No\", # \"Yes\", \"No\"\n#   'batch_normalization' : \"No\", # \"Yes\", \"No\"\n#   'dropout' : 0.2,\n#   'filter_organisation' : 'same', #'same', 'double', 'half'\n#   'total_filters': 8, #total filters on each layer\n#   'filter_size' : 3, #each filter size\n#   'stride' : 1, #stride value for filter\n#   'padding' : 0, #padding value\n#   'filter_pool': 3, #pooled filter size(filter_pool_size not needed)\n#   'stride_pool' : 1, #stride value during pooling\n#   'padding_pool' : 0, #padding value for pooling\n#   'activation' : 'ReLU', # ReLU, GELU, SiLU, Mish,\n#   'dense_layer_size' : 16,\n#   'batch_size' : 8,\n#   'image_size' : 256,\n#   'epochs':2,\n#   'learning_rate' : 0.0001,\n#   'optimizer' : 'adam',\n# }\n  class_set , load_train , load_test ,load_validation = dataset_loader(settings['image_size'] , settings['data_augmentation'] , settings['batch_size'], 0.125)\n\n\n  train_model( settings  , class_set , load_train , load_test ,load_validation)\n","metadata":{"id":"w_4laTVWmXNG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project = 'DL_assignment_2')\nwandb.agent(sweep_id, main)\nwandb.finish()","metadata":{"id":"3OYLR3aqDJo8","outputId":"0babb2a6-d9f1-4a96-8e8b-b81e018fe03d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pl epoch run\n#validation run\n#val accuracy print\n#test images plotting\n#sweep run\n#partB","metadata":{"id":"rtPOrNhIa_vA","outputId":"1df94eb8-e619-4c0a-e5ef-8edaaeb4fbfe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ps-HpKH0WSmy"},"execution_count":null,"outputs":[]}]}