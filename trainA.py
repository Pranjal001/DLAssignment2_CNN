# -*- coding: utf-8 -*-
"""parta_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-zVuThv4uUpWe2l6Ql4Zq-HKw_XTxe9H
"""

import torch.nn as nn
from torch.nn import functional as funct
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split
import os
from PIL import Image
from torch import optim
import torch
from torch.nn import Module
from torch.nn import Conv2d
from torch.nn import Linear
from torch.nn import MaxPool2d
from torch.nn import ReLU
from torch.nn import LogSoftmax
from torch import flatten
import matplotlib.pyplot as plt
import numpy as np
import argparse


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class CNN(nn.Module):

  def __init__(self,settings):
    super().__init__()
    input_channel = settings['input_channel']
    output_size = settings['output_size']
    total_filters = settings['total_filters']
    filter_size = settings['filter_size']

    filter_list_out = self.organise_filter_function(settings['filter_organisation'],total_filters)

    stride_filter = settings['stride']
    padding_filter = settings['padding']
    filter_pool = settings['filter_pool']
    padding_pool = settings['padding_pool']
    stride_pool = settings['stride_pool']
    activation = settings['activation']

    activation = self.activation_call(activation)

    dense_layer_size = settings['dense_layer_size']
    image_size = settings['image_size']
    dropout_prob = settings['dropout']
    self.batch_normalisation = settings['batch_normalisation']

# batct normalisation
    if(self.batch_normalisation == 'Yes'):
      self.conv1_bn = nn.BatchNorm2d(filter_list_out[0])
      self.conv2_bn = nn.BatchNorm2d(filter_list_out[1])
      self.conv3_bn = nn.BatchNorm2d(filter_list_out[2])
      self.conv4_bn = nn.BatchNorm2d(filter_list_out[3])
      self.conv5_bn = nn.BatchNorm2d(filter_list_out[4])

# generating convolution layer and pooling layers along with the dense layer with appropriate sizes

    #-------------------------------------------------------------------------------CONV1--------------------------------------------------------------------------------------------

    self.conv1 = nn.Conv2d(in_channels = input_channel, out_channels =filter_list_out[0], kernel_size = filter_size[0], padding = padding_filter, stride = stride_filter)
    self.activation1 = activation
    self.pool1 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)

    image_size =  self.compute_conv_size(image_size , filter_size[0] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)

    #--------------------------------------------------------------------------------CONV2---------------------------------------------------------------------------------------

    self.conv2 = nn.Conv2d(in_channels = filter_list_out[0], out_channels =filter_list_out[1], kernel_size = filter_size[1], padding = padding_filter, stride = stride_filter)
    self.activation2 = activation
    self.pool2 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)

    image_size =  self.compute_conv_size(image_size , filter_size[1] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)

    #---------------------------------------------------------------------------------CONV3------------------------------------------------------------------------------------------

    self.conv3 = nn.Conv2d(in_channels = filter_list_out[1], out_channels =filter_list_out[2], kernel_size = filter_size[2], padding = padding_filter, stride = stride_filter)
    self.activation3 = activation
    self.pool3 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)

    image_size =  self.compute_conv_size(image_size , filter_size[2] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)

    #----------------------------------------------------------------------------------CONV4-----------------------------------------------------------------------------------------

    self.conv4 = nn.Conv2d(in_channels = filter_list_out[2], out_channels =filter_list_out[3], kernel_size = filter_size[3], padding = padding_filter, stride = stride_filter)
    self.activation4 = activation
    self.pool4 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)

    image_size =  self.compute_conv_size(image_size , filter_size[3] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)

    #-----------------------------------------------------------------------------------CONV5----------------------------------------------------------------------------------------

    self.conv5 = nn.Conv2d(in_channels = filter_list_out[3], out_channels = filter_list_out[4], kernel_size = filter_size[4], padding = padding_filter, stride = stride_filter)
    self.activation5 = activation
    self.pool5 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)

    image_size =  self.compute_conv_size(image_size , filter_size[4] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)

    #-----------------------------------------------------------------------------------DENSE_LAYER-------------------------------------------------------------------------------------
    self.flatten = nn.Flatten()
    self.dropout = nn.Dropout(dropout_prob)
    self.FC1 = nn.Linear(filter_list_out[4]*image_size*image_size,dense_layer_size)
    self.activationfc1 = activation
    self.op_layer = nn.Linear(dense_layer_size,output_size)

#=============================================================================== Utility Functions ========================================================================================

  def organise_filter_function(self, org_type, filter_size):

    if(org_type == 'same'):
      factor = 1
    if(org_type == 'double'):
      factor = 2
    if(org_type == 'half'):
      factor = 0.5

    filter_list = []
    for i in range(5):
      filter_list.append(filter_size)
      filter_size = int(filter_size*factor)

    return filter_list

  def activation_call(self, act_type):

    if act_type == 'ReLU':
      return nn.ReLU()
    if act_type == 'GELU':
      return nn.GELU()
    if act_type == 'SiLU':
      return nn.SiLU()
    if act_type == 'Mish':
      return nn.Mish()

  def compute_conv_size(self,W, F, S, P, DP, FP, PP, SP):
    W =  (W - F + 2*P)//S + 1
    return (W + 2*PP - (DP* (FP-1)) - 1)//SP + 1



  def forward(self,data):

    data = self.conv1(data)
    if(self.batch_normalisation == 'Yes'):
      data = self.conv1_bn(data)
    data = self.activation1(data)
    data = self.pool1(data)

    data = self.conv2(data)
    if(self.batch_normalisation == 'Yes'):
      data = self.conv2_bn(data)
    data = self.activation2(data)
    data = self.pool2(data)

    data = self.conv3(data)
    if(self.batch_normalisation == 'Yes'):
      data = self.conv3_bn(data)
    data = self.activation3(data)
    data = self.pool3(data)

    data = self.conv4(data)
    if(self.batch_normalisation == 'Yes'):
      data = self.conv4_bn(data)
    data = self.activation4(data)
    data = self.pool4(data)

    data = self.conv5(data)
    if(self.batch_normalisation == 'Yes'):
      data = self.conv5_bn(data)
    data = self.activation5(data)
    data = self.pool5(data)

    data = self.flatten(data)
    data = self.dropout(data)
    data = self.FC1(data)
    data = self.activationfc1(data)
    data = self.op_layer(data)
    data = funct.softmax(data,dim = 1)

    return data

def dataset_loader(image_size , aug_type , batch_size,split_ratio , data_path):

  test_trans = transforms.Compose([transforms.Resize((image_size, image_size)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])

  if(aug_type == 'Yes'):

    train_trans = transforms.Compose([transforms.RandomHorizontalFlip(),transforms.RandomRotation(degrees=30),transforms.Resize((image_size, image_size)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])

  else:
    train_trans = test_trans

  dataset = data_path
  train = datasets.ImageFolder(os.path.join(dataset, 'train'), transform = train_trans)
  test = datasets.ImageFolder(os.path.join(dataset, 'val'), transform = test_trans)

  class_set = train.classes

  val = 1999
  tr = 9999-val
  train,validation = random_split(train, [tr, val])

  load_train = DataLoader(train, batch_size = batch_size, num_workers=4)
  load_validation = DataLoader(validation, batch_size = batch_size, num_workers=4)
  load_test = DataLoader(test, batch_size = batch_size, num_workers=4)


  return class_set , load_train , load_test ,load_validation

def fetch_three_images_per_class(test_loader):
    class_images = [[] for _ in range(len(test_loader.dataset.classes))]
    actual_labels = []
    class_image_count = [0] * len(class_images)

    with torch.no_grad():
        for images, labels in test_loader:
            for image, label in zip(images, labels):
                if class_image_count[label] < 3:
                    class_images[label].append(image)
                    actual_labels.append(label)
                    class_image_count[label] += 1

    print(actual_labels)
    return class_images, actual_labels

def predict_labels(model, test_loader):
    model.eval()
    predicted_labels = []
    with torch.no_grad():
        for images, _ in test_loader:
            images = images.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            predicted_labels.extend(predicted.cpu().numpy())
    return predicted_labels

def fetch_and_predict(model, test_loader):
    predicted_labels = predict_labels(model, test_loader)
    class_images,actual_labels = fetch_three_images_per_class(test_loader)
    return predicted_labels, class_images , actual_labels


def plot_predicted_and_actual(class_images, predicted_labels, actual_labels, label_names):
    fig, axs = plt.subplots(10, 3, figsize=(15, 30))
    fig.tight_layout(pad=3.0)

    for i in range(len(class_images)):
        for j in range(3):
            image = class_images[i][j].permute(1, 2, 0)
            predicted_label = label_names[predicted_labels[i * 3 + j]]
            actual_label = label_names[actual_labels[i*3 + j]]
            axs[i, j].imshow(image)
            axs[i, j].set_title(f"Actual: {actual_label} \n Predicted: {predicted_label}", fontsize=10, color='white', backgroundcolor='black', pad=10)
            axs[i, j].set_xticks([])
            axs[i, j].set_yticks([])
            axs[i, j].spines['top'].set_color('none')
            axs[i, j].spines['bottom'].set_color('none')
            axs[i, j].spines['left'].set_color('none')
            axs[i, j].spines['right'].set_color('none')
            axs[i, j].tick_params(axis='both', which='both', length=0)
            axs[i, j].set_aspect('auto')

    plt.show()

def train_model( settings , class_set , load_train , load_test ,load_validation ):

  def assign_optimizer(opt_type , lr , model):
    if(opt_type  == 'adam'):
      optimizer = optim.Adam(model.parameters(), lr)
      return optimizer
    if(opt_type  == 'sgd'):
      optimizer = optim.SGD(model.parameters(), lr)
      return optimizer
    if(opt_type  == 'nadam'):
      optimizer = optim.NAdam(model.parameters(), lr)
      return optimizer

  model = CNN(settings).to(device)
  optimizer = assign_optimizer(settings['optimizer'] , settings['learning_rate'] , model)
  criterion = nn.CrossEntropyLoss()

  for epochit in range(settings['epochs']):
      temp_loss_train = 0.0
      pred_train = 0
      pred_total = 0


      model.train()

      for images, labels in load_train:

          images = images.to(device)
          labels =  labels.to(device)

          optimizer.zero_grad()
          outputs = model.forward(images)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()

          temp_loss_train += loss.item()
          _, predicted = torch.max(outputs.data, 1)
          pred_total += labels.size(0)
          pred_train += (predicted == labels).sum().item()


      train_accuracy = pred_train / pred_total
      loss_train = temp_loss_train / len(load_train)
      print('===================================================================================')
      print('Epoch ',epochit+1, "Train Loss:", loss_train, "Train Accuracy:" ,train_accuracy)



      model.eval()
      temp_loss_val = 0.0
      pred_val = 0
      pred_total_val = 0

      with torch.no_grad():
          for val_images, val_labels in load_validation:

              val_images = val_images.to(device)
              val_labels =  val_labels.to(device)
              val_outputs = model.forward(val_images)
              val_loss = criterion(val_outputs, val_labels)
              temp_loss_val += val_loss.item()

              _, val_predicted = torch.max(val_outputs.data, 1)
              pred_total_val += val_labels.size(0)
              pred_val += (val_predicted == val_labels).sum().item()

      val_accuracy = pred_val / pred_total_val
      val_average_loss = temp_loss_val / len(load_validation)
      print("Validation Loss:", val_average_loss, "Validation Accuracy:" ,val_accuracy)
      print('===================================================================================')



# param = wandb.init(project="DL_assignment_2")
# wandb.run.name = (
#     ":ep-" + str(param.config.epochs) +
#     ":opt-" + param.config.optimizer +
#     ":a-" + param.config.activation +
#     ":bs-" + str(param.config.batch_size) +
#     ":fs-" + str(param.config.filter_size) +
#     ":fp-" + str(param.config.filter_pool) +
#     ":da-" + param.config.data_augmentation +
#     ":eta-" + str(param.config.learning_rate) +
#     ":dls-" + str( param.config.dense_layer_size)
# )

# settings = {
#     'input_channel' : param.config.input_channel,
#     'output_size' : param.config.output_size,
#     'data_augmentation' : param.config.data_augmentation, # "Yes", "No"
#     'batch_normalisation' : param.config.batch_normalisation, # "Yes", "No"
#     'dropout' : param.config.dropout,
#     'filter_organisation' : param.config.filter_organisation, #'same', 'double', 'half'
#     'total_filters': param.config.total_filters, #total filters on each layer
#     'filter_size' : param.config.filter_size, #each filter size
#     'stride' : param.config.stride, #stride value for filter
#     'padding' : param.config.padding, #padding value
#     'filter_pool': param.config.filter_pool, #pooled filter size(filter_pool_size not needed)
#     'stride_pool' : param.config.stride_pool, #stride value during pooling
#     'padding_pool' : param.config.padding_pool, #padding value for pooling
#     'activation' : param.config.activation, # ReLU, GELU, SiLU, Mish,
#     'dense_layer_size' : param.config.dense_layer_size,
#     'batch_size' : param.config.batch_size,
#     'image_size' : param.config.image_size,
#     'epochs':param.config.epochs,
#     'learning_rate' : param.config.learning_rate,
#     'optimizer' : param.config.optimizer,

# }

def parse_arguments():
  parser = argparse.ArgumentParser(description="CNN")
  parser.add_argument('-in', '--input_channel', type=int, default=3, help='Number of input channels')
  parser.add_argument('-op', '--output_size', type=int, default=10, help='Number of output neurons')
  parser.add_argument('-ep', '--epochs', type=int, default=10, help='Number of epochs')
  parser.add_argument('-dls', '--dense_layer_size', type=int, default=256, help='Size of the dense layer i.e number of neurons in FC layer')
  parser.add_argument('-fo', '--filter_organisation', type=str, default='half', help='Filter organization same,double, half')
  parser.add_argument('-opt', '--optimizer', type=str, default='adam', help='Optimizer adam or sgd or nadam')
  parser.add_argument('-tf', '--total_filters', type=int, default=128, help='total filters on each layer')
  parser.add_argument('-act', '--activation', type=str, default='ReLU', help='Activation function type')
  parser.add_argument('-s', '--stride', type=int, default=1, help='Stride size same for each layer')
  parser.add_argument('-fs', '--filter_size', type=int, default=[3]*5, help='Filter size list')
  parser.add_argument('-pf', '--filter_pool', type=int, default=1, help='Pooling filter sizes')
  parser.add_argument('-p', '--padding', type=int,default=0, help='padding sizes same for each layer')
  parser.add_argument('-ps', '--stride_pool', default=1, help='Pooling stride sizes')
  parser.add_argument('-pp', '--padding_pool', type=int, default=0, help='Pooling padding sizes')
  parser.add_argument('-aug', '--data_augmentation', type=str, default='No', help='data augmentation (Yes/No)')
  parser.add_argument('-bn', '--batch_normalisation', type=str, default='Yes', help='batch normalization (Yes/No)')
  parser.add_argument('-d', '--dropout', type=float, default=0.1, help='Dropout probability value')
  parser.add_argument('-bs', '--batch_size', type=int, default=32, help='Batch size')
  parser.add_argument('-img', '--image_size', type=int, default=224, help='Image size')
  parser.add_argument('-lr', '--learning_rate', type=float, default=0.0001, help='Learning rate')
  parser.add_argument('-dp', '--data_path', type=str, default='/content/inaturalist_12K', help='Path to data directory')
  return parser.parse_args()




arguments = parse_arguments()
settings = {
  'input_channel' : arguments.input_channel,
  'output_size' : arguments.output_size,
  'data_augmentation' : arguments.data_augmentation, # "Yes", "No"
  'batch_normalisation' : arguments.batch_normalisation, # "Yes", "No"
  'dropout' : arguments.dropout,
  'filter_organisation' : arguments.filter_organisation, #'same', 'double', 'half'
  'total_filters': arguments.total_filters, #total filters on each layer
  'filter_size' : arguments.filter_size, #each filter size
  'stride' : arguments.stride, #stride value for filter
  'padding' : arguments.padding, #padding value
  'filter_pool': arguments.filter_pool, #pooled filter size(filter_pool_size not needed)
  'stride_pool' : arguments.stride_pool, #stride value during pooling
  'padding_pool' : arguments.padding_pool, #padding value for pooling
  'activation' : arguments.activation, # ReLU, GELU, SiLU, Mish,
  'dense_layer_size' : arguments.dense_layer_size,
  'batch_size' : arguments.batch_size,
  'image_size' : arguments.image_size,
  'epochs':arguments.epochs,
  'learning_rate' : arguments.learning_rate,
  'optimizer' : arguments.optimizer,
}
class_set , load_train , load_test ,load_validation = dataset_loader(settings['image_size'] , settings['data_augmentation'] , settings['batch_size'], 0.125 ,data_path =  arguments.data_path)
train_model( settings  , class_set , load_train , load_test ,load_validation)

