{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4502aa019d3c4baf8b3fa152616c251d":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_db8c8df80cb14c11a80cebad8c21248d","IPY_MODEL_787c56a233d041feb960afaea800226a"],"layout":"IPY_MODEL_f511510faf934ea18a010e8186069114"}},"db8c8df80cb14c11a80cebad8c21248d":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd14b4eae8104e72aea1141581169a4e","placeholder":"​","style":"IPY_MODEL_b4d832c35b0b4632b8fb3ec4f7157e43","value":"0.012 MB of 0.012 MB uploaded\r"}},"787c56a233d041feb960afaea800226a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a788cb59d9fd42dfb1c5e7477e74587f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c465fcf99139492a85528626425dcf78","value":1}},"f511510faf934ea18a010e8186069114":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd14b4eae8104e72aea1141581169a4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4d832c35b0b4632b8fb3ec4f7157e43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a788cb59d9fd42dfb1c5e7477e74587f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c465fcf99139492a85528626425dcf78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8030211,"sourceType":"datasetVersion","datasetId":4733049}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb","metadata":{"id":"jpyDr-KJp_tu","outputId":"6bb710fc-2b58-4b1e-e757-f1a20534e985","execution":{"iopub.status.busy":"2024-04-05T23:51:40.489555Z","iopub.execute_input":"2024-04-05T23:51:40.489985Z","iopub.status.idle":"2024-04-05T23:51:54.449669Z","shell.execute_reply.started":"2024-04-05T23:51:40.489951Z","shell.execute_reply":"2024-04-05T23:51:54.448640Z"},"editable":false,"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.4)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.42.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\n!wandb login '9a5ccd0848378e80e2abf8c49fbe7f9d7c5e0b10'","metadata":{"id":"IfXdtk18qBiR","outputId":"b2034043-b5b6-4a17-b93f-dc201536cc5f","execution":{"iopub.status.busy":"2024-04-05T23:51:54.451657Z","iopub.execute_input":"2024-04-05T23:51:54.451966Z","iopub.status.idle":"2024-04-05T23:51:58.460525Z","shell.execute_reply.started":"2024-04-05T23:51:54.451937Z","shell.execute_reply":"2024-04-05T23:51:58.459281Z"},"editable":false,"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.nn import functional as funct\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nimport os\nfrom PIL import Image\nfrom torch import optim\nimport torch\nfrom torch.nn import Module\nfrom torch.nn import Conv2d\nfrom torch.nn import Linear\nfrom torch.nn import MaxPool2d\nfrom torch.nn import ReLU\nfrom torch.nn import LogSoftmax\nfrom torch import flatten","metadata":{"id":"8BshsEtqc3jH","execution":{"iopub.status.busy":"2024-04-05T23:51:58.462174Z","iopub.execute_input":"2024-04-05T23:51:58.462499Z","iopub.status.idle":"2024-04-05T23:52:05.111241Z","shell.execute_reply.started":"2024-04-05T23:51:58.462465Z","shell.execute_reply":"2024-04-05T23:52:05.110422Z"},"editable":false,"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"t7ngzSyLpT-l","execution":{"iopub.status.busy":"2024-04-05T23:52:05.113320Z","iopub.execute_input":"2024-04-05T23:52:05.113968Z","iopub.status.idle":"2024-04-05T23:52:05.169969Z","shell.execute_reply.started":"2024-04-05T23:52:05.113939Z","shell.execute_reply":"2024-04-05T23:52:05.168762Z"},"editable":false,"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# !pip install wget\n# import wget\n# wget.download('https://storage.googleapis.com/wandb_datasets/nature_12K.zip')\n# !unzip /kaggle/working/nature_12K.zip","metadata":{"id":"iGpArvflabm0","outputId":"33c7cffa-9bc9-4d02-a4a7-dde12c3f320d","execution":{"iopub.status.busy":"2024-04-05T23:52:05.171458Z","iopub.execute_input":"2024-04-05T23:52:05.172140Z","iopub.status.idle":"2024-04-05T23:52:05.183857Z","shell.execute_reply.started":"2024-04-05T23:52:05.172104Z","shell.execute_reply":"2024-04-05T23:52:05.182838Z"},"editable":false,"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n\n  def __init__(self,settings):\n    super().__init__()\n    input_channel = settings['input_channel']\n    output_size = settings['output_size']\n    total_filters = settings['total_filters']\n    filter_size = settings['filter_size']\n\n    filter_list_out = self.organise_filter_function(settings['filter_organisation'],total_filters)\n\n    stride_filter = settings['stride']\n    padding_filter = settings['padding']\n    filter_pool = settings['filter_pool']\n    padding_pool = settings['padding_pool']\n    stride_pool = settings['stride_pool']\n    activation = settings['activation']\n\n    activation = self.activation_call(activation)\n\n    dense_layer_size = settings['dense_layer_size']\n    image_size = settings['image_size']\n    dropout_prob = settings['dropout']\n    self.batch_normalisation = settings['batch_normalisation']\n\n# batct normalisation\n    if(self.batch_normalisation == 'Yes'):\n      self.conv1_bn = nn.BatchNorm2d(filter_list_out[0])\n      self.conv2_bn = nn.BatchNorm2d(filter_list_out[1])\n      self.conv3_bn = nn.BatchNorm2d(filter_list_out[2])\n      self.conv4_bn = nn.BatchNorm2d(filter_list_out[3])\n      self.conv5_bn = nn.BatchNorm2d(filter_list_out[4])\n\n# generating convolution layer and pooling layers along with the dense layer with appropriate sizes\n\n    #-------------------------------------------------------------------------------CONV1--------------------------------------------------------------------------------------------\n\n    self.conv1 = nn.Conv2d(in_channels = input_channel, out_channels =filter_list_out[0], kernel_size = filter_size[0], padding = padding_filter, stride = stride_filter)\n    self.activation1 = activation\n    self.pool1 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)\n\n    image_size =  self.compute_conv_size(image_size , filter_size[0] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)\n\n    #--------------------------------------------------------------------------------CONV2---------------------------------------------------------------------------------------\n\n    self.conv2 = nn.Conv2d(in_channels = filter_list_out[0], out_channels =filter_list_out[1], kernel_size = filter_size[1], padding = padding_filter, stride = stride_filter)\n    self.activation2 = activation\n    self.pool2 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)\n\n    image_size =  self.compute_conv_size(image_size , filter_size[1] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)\n\n    #---------------------------------------------------------------------------------CONV3------------------------------------------------------------------------------------------\n\n    self.conv3 = nn.Conv2d(in_channels = filter_list_out[1], out_channels =filter_list_out[2], kernel_size = filter_size[2], padding = padding_filter, stride = stride_filter)\n    self.activation3 = activation\n    self.pool3 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)\n\n    image_size =  self.compute_conv_size(image_size , filter_size[2] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)\n\n    #----------------------------------------------------------------------------------CONV4-----------------------------------------------------------------------------------------\n\n    self.conv4 = nn.Conv2d(in_channels = filter_list_out[2], out_channels =filter_list_out[3], kernel_size = filter_size[3], padding = padding_filter, stride = stride_filter)\n    self.activation4 = activation\n    self.pool4 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)\n\n    image_size =  self.compute_conv_size(image_size , filter_size[3] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)\n\n    #-----------------------------------------------------------------------------------CONV5----------------------------------------------------------------------------------------\n\n    self.conv5 = nn.Conv2d(in_channels = filter_list_out[3], out_channels = filter_list_out[4], kernel_size = filter_size[4], padding = padding_filter, stride = stride_filter)\n    self.activation5 = activation\n    self.pool5 = nn.MaxPool2d(kernel_size = filter_pool, padding = padding_pool, stride = stride_pool)\n\n    image_size =  self.compute_conv_size(image_size , filter_size[4] ,stride_filter , padding_filter , 1 , filter_pool, padding_pool, stride_pool)\n\n    #-----------------------------------------------------------------------------------DENSE_LAYER-------------------------------------------------------------------------------------\n    self.flatten = nn.Flatten()\n    self.dropout = nn.Dropout(dropout_prob)\n    self.FC1 = nn.Linear(filter_list_out[4]*image_size*image_size,dense_layer_size)\n    self.activationfc1 = activation\n    self.op_layer = nn.Linear(dense_layer_size,output_size)\n\n#=============================================================================== Utility Functions ========================================================================================\n\n  def organise_filter_function(self, org_type, filter_size):\n\n    if(org_type == 'same'):\n      factor = 1\n    if(org_type == 'double'):\n      factor = 2\n    if(org_type == 'half'):\n      factor = 0.5\n\n    filter_list = []\n    for i in range(5):\n      filter_list.append(filter_size)\n      filter_size = int(filter_size*factor)\n\n    return filter_list\n\n  def activation_call(self, act_type):\n\n    if act_type == 'ReLU':\n      return nn.ReLU()\n    if act_type == 'GELU':\n      return nn.GELU()\n    if act_type == 'SiLU':\n      return nn.SiLU()\n    if act_type == 'Mish':\n      return nn.Mish()\n\n  def compute_conv_size(self,W, F, S, P, DP, FP, PP, SP):\n    W =  (W - F + 2*P)//S + 1\n    return (W + 2*PP - (DP* (FP-1)) - 1)//SP + 1\n\n\n\n  def forward(self,data):\n\n    data = self.conv1(data)\n    if(self.batch_normalisation == 'Yes'):\n      data = self.conv1_bn(data)\n    data = self.activation1(data)\n    data = self.pool1(data)\n\n    data = self.conv2(data)\n    if(self.batch_normalisation == 'Yes'):\n      data = self.conv2_bn(data)\n    data = self.activation2(data)\n    data = self.pool2(data)\n\n    data = self.conv3(data)\n    if(self.batch_normalisation == 'Yes'):\n      data = self.conv3_bn(data)\n    data = self.activation3(data)\n    data = self.pool3(data)\n\n    data = self.conv4(data)\n    if(self.batch_normalisation == 'Yes'):\n      data = self.conv4_bn(data)\n    data = self.activation4(data)\n    data = self.pool4(data)\n\n    data = self.conv5(data)\n    if(self.batch_normalisation == 'Yes'):\n      data = self.conv5_bn(data)\n    data = self.activation5(data)\n    data = self.pool5(data)\n\n    data = self.flatten(data)\n    data = self.dropout(data)\n    data = self.FC1(data)\n    data = self.activationfc1(data)\n    data = self.op_layer(data)\n    data = funct.softmax(data,dim = 1)\n\n    return data\n\n\n","metadata":{"id":"-eK_fT1vfr3h","execution":{"iopub.status.busy":"2024-04-05T23:52:05.185612Z","iopub.execute_input":"2024-04-05T23:52:05.185956Z","iopub.status.idle":"2024-04-05T23:52:05.217399Z","shell.execute_reply.started":"2024-04-05T23:52:05.185924Z","shell.execute_reply":"2024-04-05T23:52:05.216520Z"},"editable":false,"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def dataset_loader(image_size , aug_type , batch_size,split_ratio):\n\n  test_trans = transforms.Compose([transforms.Resize((image_size, image_size)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n\n  if(aug_type == 'Yes'):\n\n    train_trans = transforms.Compose([transforms.RandomHorizontalFlip(),transforms.RandomRotation(degrees=30),transforms.Resize((image_size, image_size)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n\n  else:\n    train_trans = test_trans\n\n  dataset = \"/kaggle/input/naturalistdata/inaturalist_12K\"\n  train = datasets.ImageFolder(os.path.join(dataset, 'train'), transform = train_trans)\n  test = datasets.ImageFolder(os.path.join(dataset, 'val'), transform = test_trans)\n\n  class_set = train.classes\n\n  val = 1999\n  tr = 9999-val\n  train,validation = random_split(train, [tr, val])\n\n  load_train = DataLoader(train, batch_size = batch_size, num_workers=4)\n  load_validation = DataLoader(validation, batch_size = batch_size, num_workers=4)\n  load_test = DataLoader(test, batch_size = batch_size, num_workers=4)\n\n\n  return class_set , load_train , load_test ,load_validation\n\n","metadata":{"id":"3REjJ3mCeWnz","execution":{"iopub.status.busy":"2024-04-05T23:52:05.218524Z","iopub.execute_input":"2024-04-05T23:52:05.218840Z","iopub.status.idle":"2024-04-05T23:52:05.231261Z","shell.execute_reply.started":"2024-04-05T23:52:05.218815Z","shell.execute_reply":"2024-04-05T23:52:05.230509Z"},"editable":false,"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#-------------------------------plotting---------------------\ndef fetch_three_images_per_class(test_loader):\n    class_images = [[] for _ in range(len(test_loader.dataset.classes))]\n    actual_labels = []\n    class_image_count = [0] * len(class_images)\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            for image, label in zip(images, labels):\n                if class_image_count[label] < 3:\n                    class_images[label].append(image)\n                    actual_labels.append(label)\n                    class_image_count[label] += 1\n\n    print(actual_labels)\n    return class_images, actual_labels\n\ndef predict_labels(model, test_loader):\n    model.eval()\n    predicted_labels = []\n    with torch.no_grad():\n        for images, _ in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            predicted_labels.extend(predicted.cpu().numpy())\n    return predicted_labels\n\ndef fetch_and_predict(model, test_loader):\n    predicted_labels = predict_labels(model, test_loader)\n    class_images,actual_labels = fetch_three_images_per_class(test_loader)\n    return predicted_labels, class_images , actual_labels\n\n\ndef plot_predicted_and_actual(class_images, predicted_labels, actual_labels, label_names):\n    fig, axs = plt.subplots(10, 3, figsize=(15, 30))\n    fig.tight_layout(pad=3.0)\n\n    for i in range(len(class_images)):\n        for j in range(3):\n            image = class_images[i][j].permute(1, 2, 0)\n            predicted_label = label_names[predicted_labels[i * 3 + j]]\n            actual_label = label_names[actual_labels[i*3 + j]]\n            axs[i, j].imshow(image)\n            axs[i, j].set_title(f\"Actual: {actual_label} \\n Predicted: {predicted_label}\", fontsize=10, color='white', backgroundcolor='black', pad=10)\n            axs[i, j].set_xticks([])\n            axs[i, j].set_yticks([])\n            axs[i, j].spines['top'].set_color('none')\n            axs[i, j].spines['bottom'].set_color('none')\n            axs[i, j].spines['left'].set_color('none')\n            axs[i, j].spines['right'].set_color('none')\n            axs[i, j].tick_params(axis='both', which='both', length=0)\n            axs[i, j].set_aspect('auto')\n            wandb.log({\"Plot\": [wandb.Image(axes[i][j], caption=f\"Actual: {actual_label} \\n Predicted: {predicted_label}\")]})\n\n    plt.show()\n    wandb.finish()\n","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sweep_config = {\n    'name' : \"sweep_best1\",\n    'method': 'random'\n    }\n\n# parameters_sweep = {\n\n#   'input_channel' : {'values' : [3] },\n#   'output_size' : {'values' : [10] },\n#   'data_augmentation' : {'values' : ['Yes','No'] }, # \"Yes\", \"No\"\n#   'batch_normalisation' : {'values' : ['Yes','No'] }, # \"Yes\", \"No\"\n#   'dropout' : {'values' : [0.1,0.3] },\n#   'filter_organisation' : {'values' : ['same','double'] }, #'same', 'double', 'half'\n#   'total_filters': {'values' : [32,64,128] }, #total filters on each layer\n#   'filter_size' : {'values' : [[7,5,5,3,3],[11,7,5,3,3],[3,3,3,3,3]] }, #each filter size\n#   'stride' : {'values' : [1,2] }, #stride value for filter\n#   'padding' : {'values' : [0] }, #padding value\n#   'filter_pool': {'values' : [3,2] }, \n#   'stride_pool' : {'values' : [1,2] }, #stride value during pooling\n#   'padding_pool' : {'values' : [0] }, #padding value for pooling\n#   'activation' : {'values' : ['ReLU', 'GELU', 'Mish'] }, # ReLU, GELU, SiLU, Mish,\n#   'dense_layer_size' : {'values' : [256,1024] },\n#   'batch_size' : {'values' : [8,64] },\n#   'image_size' : {'values' : [224,256] },\n#   'epochs':{'values' : [10,15] },\n#   'learning_rate' : {'values' : [0.0001,0.0003] },\n#   'optimizer' : {'values' : ['adam'] },\n#   }\n\nparameters_sweep = {\n\n  'input_channel' : {'values' : [3] },\n  'output_size' : {'values' : [10] },\n  'data_augmentation' : {'values' : ['No'] }, # \"Yes\", \"No\"\n  'batch_normalisation' : {'values' : ['Yes'] }, # \"Yes\", \"No\"\n  'dropout' : {'values' : [0.2] },\n  'filter_organisation' : {'values' : ['same'] }, #'same', 'double', 'half'\n  'total_filters': {'values' : [128] }, #total filters on each layer\n  'filter_size' : {'values' : [[7,7,7,7,7]] }, #each filter size\n  'stride' : {'values' : [2] }, #stride value for filter\n  'padding' : {'values' : [0] }, #padding value\n  'filter_pool': {'values' : [2] }, \n  'stride_pool' : {'values' : [1] }, #stride value during pooling\n  'padding_pool' : {'values' : [0] }, #padding value for pooling\n  'activation' : {'values' : ['GELU'] }, # ReLU, GELU, SiLU, Mish,\n  'dense_layer_size' : {'values' : [1024] },\n  'batch_size' : {'values' : [32] },\n  'image_size' : {'values' : [256] },\n  'epochs':{'values' : [10] },\n  'learning_rate' : {'values' : [0.0003] },\n  'optimizer' : {'values' : ['adam'] },\n  }\n\n\n\nmetric = {\n    'name' : 'Accuracy',\n    'goal' : 'maximize'\n}\n\nsweep_config['metric'] = metric\n\nsweep_config['parameters'] = parameters_sweep","metadata":{"id":"963zPizCrSNr","execution":{"iopub.status.busy":"2024-04-05T23:58:49.023037Z","iopub.execute_input":"2024-04-05T23:58:49.024377Z","iopub.status.idle":"2024-04-05T23:58:49.038057Z","shell.execute_reply.started":"2024-04-05T23:58:49.024325Z","shell.execute_reply":"2024-04-05T23:58:49.036961Z"},"editable":false,"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def train_model( settings , class_set , load_train , load_test ,load_validation ):\n\n  def assign_optimizer(opt_type , lr , model):\n    if(opt_type  == 'adam'):\n      optimizer = optim.Adam(model.parameters(), lr)\n      return optimizer\n    if(opt_type  == 'sgd'):\n      optimizer = optim.SGD(model.parameters(), lr)\n      return optimizer\n    if(opt_type  == 'nadam'):\n      optimizer = optim.NAdam(model.parameters(), lr)\n      return optimizer\n  \n  model = CNN(settings).to(device)\n  optimizer = assign_optimizer(settings['optimizer'] , settings['learning_rate'] , model)\n  criterion = nn.CrossEntropyLoss()\n\n  for epochit in range(settings['epochs']):\n      temp_loss_train = 0.0\n      pred_train = 0\n      pred_total = 0\n\n\n      model.train()\n\n      for images, labels in load_train:\n\n          images = images.to(device)\n          labels =  labels.to(device)\n\n          optimizer.zero_grad()\n          outputs = model.forward(images)\n          loss = criterion(outputs, labels)\n          loss.backward()\n          optimizer.step()\n\n          temp_loss_train += loss.item()\n          _, predicted = torch.max(outputs.data, 1)\n          pred_total += labels.size(0)\n          pred_train += (predicted == labels).sum().item()\n\n\n      train_accuracy = pred_train / pred_total\n      loss_train = temp_loss_train / len(load_train)\n      print('===================================================================================')\n      print('Epoch ',epochit+1, \"Train Loss:\", loss_train, \"Train Accuracy:\" ,train_accuracy)\n      wandb.log({'epochs':epochit + 1, 'train_loss': loss_train, 'train_accuracy': train_accuracy})\n\n\n\n      model.eval()\n      temp_loss_val = 0.0\n      pred_val = 0\n      pred_total_val = 0\n\n      with torch.no_grad():\n          for val_images, val_labels in load_validation:\n\n              val_images = val_images.to(device)\n              val_labels =  val_labels.to(device)\n              val_outputs = model.forward(val_images)\n              val_loss = criterion(val_outputs, val_labels)\n              temp_loss_val += val_loss.item()\n\n              _, val_predicted = torch.max(val_outputs.data, 1)\n              pred_total_val += val_labels.size(0)\n              pred_val += (val_predicted == val_labels).sum().item()\n\n      val_accuracy = pred_val / pred_total_val\n      val_average_loss = temp_loss_val / len(load_validation)\n      wandb.log({'val_loss': val_average_loss, 'val_accuracy': val_accuracy})\n      print(\"Validation Loss:\", val_average_loss, \"Validation Accuracy:\" ,val_accuracy)\n      print('===================================================================================')\n\n\n  wandb.log({'Accuracy' :val_accuracy})\n\npredicted_labels, class_images,actual_labels= fetch_and_predict(model, load_test)\nplot_predicted_and_actual(class_images, predicted_labels, actual_labels, load_test.dataset.classes)\n\n\n\n\n","metadata":{"id":"3B6A6I7_nx5C","execution":{"iopub.status.busy":"2024-04-05T23:52:05.246372Z","iopub.execute_input":"2024-04-05T23:52:05.246954Z","iopub.status.idle":"2024-04-05T23:52:05.262676Z","shell.execute_reply.started":"2024-04-05T23:52:05.246921Z","shell.execute_reply":"2024-04-05T23:52:05.261831Z"},"editable":false,"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def main():\n  param = wandb.init(project=\"DL_assignment_2\")\n  wandb.run.name = (\n      \":ep-\" + str(param.config.epochs) +\n      \":opt-\" + param.config.optimizer +\n      \":a-\" + param.config.activation +\n      \":bs-\" + str(param.config.batch_size) +\n      \":fs-\" + str(param.config.filter_size) +\n      \":fp-\" + str(param.config.filter_pool) +\n      \":da-\" + param.config.data_augmentation +\n      \":eta-\" + str(param.config.learning_rate) +\n      \":dls-\" + str( param.config.dense_layer_size)\n  )\n\n  settings = {\n      'input_channel' : param.config.input_channel,\n      'output_size' : param.config.output_size,\n      'data_augmentation' : param.config.data_augmentation, # \"Yes\", \"No\"\n      'batch_normalisation' : param.config.batch_normalisation, # \"Yes\", \"No\"\n      'dropout' : param.config.dropout,\n      'filter_organisation' : param.config.filter_organisation, #'same', 'double', 'half'\n      'total_filters': param.config.total_filters, #total filters on each layer\n      'filter_size' : param.config.filter_size, #each filter size\n      'stride' : param.config.stride, #stride value for filter\n      'padding' : param.config.padding, #padding value\n      'filter_pool': param.config.filter_pool, #pooled filter size(filter_pool_size not needed)\n      'stride_pool' : param.config.stride_pool, #stride value during pooling\n      'padding_pool' : param.config.padding_pool, #padding value for pooling\n      'activation' : param.config.activation, # ReLU, GELU, SiLU, Mish,\n      'dense_layer_size' : param.config.dense_layer_size,\n      'batch_size' : param.config.batch_size,\n      'image_size' : param.config.image_size,\n      'epochs':param.config.epochs,\n      'learning_rate' : param.config.learning_rate,\n      'optimizer' : param.config.optimizer,\n\n  }\n# settings = {\n#   'input_channel' : 3,\n#   'output_size' : 10,\n#   'data_augmentation' : \"No\", # \"Yes\", \"No\"\n#   'batch_normalization' : \"No\", # \"Yes\", \"No\"\n#   'dropout' : 0.2,\n#   'filter_organisation' : 'same', #'same', 'double', 'half'\n#   'total_filters': 8, #total filters on each layer\n#   'filter_size' : 3, #each filter size\n#   'stride' : 1, #stride value for filter\n#   'padding' : 0, #padding value\n#   'filter_pool': 3, #pooled filter size(filter_pool_size not needed)\n#   'stride_pool' : 1, #stride value during pooling\n#   'padding_pool' : 0, #padding value for pooling\n#   'activation' : 'ReLU', # ReLU, GELU, SiLU, Mish,\n#   'dense_layer_size' : 16,\n#   'batch_size' : 8,\n#   'image_size' : 256,\n#   'epochs':2,\n#   'learning_rate' : 0.0001,\n#   'optimizer' : 'adam',\n# }\n  class_set , load_train , load_test ,load_validation = dataset_loader(settings['image_size'] , settings['data_augmentation'] , settings['batch_size'], 0.125)\n\n\n  train_model( settings  , class_set , load_train , load_test ,load_validation)\n","metadata":{"id":"w_4laTVWmXNG","execution":{"iopub.status.busy":"2024-04-05T23:52:05.264957Z","iopub.execute_input":"2024-04-05T23:52:05.265273Z","iopub.status.idle":"2024-04-05T23:52:05.277715Z","shell.execute_reply.started":"2024-04-05T23:52:05.265249Z","shell.execute_reply":"2024-04-05T23:52:05.276795Z"},"editable":false,"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project = 'DL_assignment_2')\nwandb.agent(sweep_id, main)\nwandb.finish()","metadata":{"id":"3OYLR3aqDJo8","outputId":"0babb2a6-d9f1-4a96-8e8b-b81e018fe03d","execution":{"iopub.status.busy":"2024-04-05T23:58:57.717068Z","iopub.execute_input":"2024-04-05T23:58:57.717692Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Create sweep with ID: 43x3m7vp\nSweep URL: https://wandb.ai/ml_panda/DL_assignment_2/sweeps/43x3m7vp\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pc9hp7qq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: GELU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_normalisation: Yes\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: No\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_layer_size: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organisation: same\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_pool: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: [7, 7, 7, 7, 7]\n\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_channel: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding_pool: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride_pool: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240405_235900-pc9hp7qq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ml_panda/DL_assignment_2/runs/pc9hp7qq' target=\"_blank\">effortless-sweep-1</a></strong> to <a href='https://wandb.ai/ml_panda/DL_assignment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ml_panda/DL_assignment_2/sweeps/43x3m7vp' target=\"_blank\">https://wandb.ai/ml_panda/DL_assignment_2/sweeps/43x3m7vp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ml_panda/DL_assignment_2' target=\"_blank\">https://wandb.ai/ml_panda/DL_assignment_2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ml_panda/DL_assignment_2/sweeps/43x3m7vp' target=\"_blank\">https://wandb.ai/ml_panda/DL_assignment_2/sweeps/43x3m7vp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ml_panda/DL_assignment_2/runs/pc9hp7qq' target=\"_blank\">https://wandb.ai/ml_panda/DL_assignment_2/runs/pc9hp7qq</a>"},"metadata":{}},{"name":"stdout","text":"===================================================================================\nEpoch  1 Train Loss: 2.2379915199279785 Train Accuracy: 0.20375\nValidation Loss: 2.211368413198562 Validation Accuracy: 0.21810905452726362\n===================================================================================\n===================================================================================\nEpoch  2 Train Loss: 2.1895472383499146 Train Accuracy: 0.259\nValidation Loss: 2.168317688835992 Validation Accuracy: 0.2801400700350175\n===================================================================================\n===================================================================================\nEpoch  3 Train Loss: 2.159132822036743 Train Accuracy: 0.293375\nValidation Loss: 2.147629287507799 Validation Accuracy: 0.3026513256628314\n===================================================================================\n===================================================================================\nEpoch  4 Train Loss: 2.139506446838379 Train Accuracy: 0.316375\nValidation Loss: 2.1406484123260254 Validation Accuracy: 0.3071535767883942\n===================================================================================\n===================================================================================\nEpoch  5 Train Loss: 2.122748327255249 Train Accuracy: 0.333375\nValidation Loss: 2.123427256705269 Validation Accuracy: 0.3281640820410205\n===================================================================================\n===================================================================================\nEpoch  6 Train Loss: 2.1088070378303527 Train Accuracy: 0.3475\nValidation Loss: 2.117964572376675 Validation Accuracy: 0.33466733366683343\n===================================================================================\n===================================================================================\nEpoch  7 Train Loss: 2.0932756547927855 Train Accuracy: 0.361625\nValidation Loss: 2.105664531389872 Validation Accuracy: 0.3461730865432716\n===================================================================================\n===================================================================================\nEpoch  8 Train Loss: 2.0865215406417845 Train Accuracy: 0.367875\nValidation Loss: 2.118927484466916 Validation Accuracy: 0.3311655827913957\n===================================================================================\n===================================================================================\nEpoch  9 Train Loss: 2.069963500499725 Train Accuracy: 0.3865\nValidation Loss: 2.1048556868992154 Validation Accuracy: 0.35167583791895946\n===================================================================================\n===================================================================================\nEpoch  10 Train Loss: 2.060307092666626 Train Accuracy: 0.39775\nValidation Loss: 2.1027136946481373 Validation Accuracy: 0.3491745872936468\n===================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>epochs</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇█▇██</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.34917</td></tr><tr><td>epochs</td><td>10</td></tr><tr><td>train_accuracy</td><td>0.39775</td></tr><tr><td>train_loss</td><td>2.06031</td></tr><tr><td>val_accuracy</td><td>0.34917</td></tr><tr><td>val_loss</td><td>2.10271</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">effortless-sweep-1</strong> at: <a href='https://wandb.ai/ml_panda/DL_assignment_2/runs/pc9hp7qq' target=\"_blank\">https://wandb.ai/ml_panda/DL_assignment_2/runs/pc9hp7qq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240405_235900-pc9hp7qq/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p8fwv8av with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: GELU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_normalisation: Yes\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: No\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_layer_size: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organisation: same\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_pool: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: [7, 7, 7, 7, 7]\n\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_channel: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding_pool: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride_pool: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240406_000919-p8fwv8av</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ml_panda/DL_assignment_2/runs/p8fwv8av' target=\"_blank\">elated-sweep-2</a></strong> to <a href='https://wandb.ai/ml_panda/DL_assignment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ml_panda/DL_assignment_2/sweeps/43x3m7vp' target=\"_blank\">https://wandb.ai/ml_panda/DL_assignment_2/sweeps/43x3m7vp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ml_panda/DL_assignment_2' target=\"_blank\">https://wandb.ai/ml_panda/DL_assignment_2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ml_panda/DL_assignment_2/sweeps/43x3m7vp' target=\"_blank\">https://wandb.ai/ml_panda/DL_assignment_2/sweeps/43x3m7vp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ml_panda/DL_assignment_2/runs/p8fwv8av' target=\"_blank\">https://wandb.ai/ml_panda/DL_assignment_2/runs/p8fwv8av</a>"},"metadata":{}},{"name":"stdout","text":"===================================================================================\nEpoch  1 Train Loss: 2.238804307937622 Train Accuracy: 0.204375\n","output_type":"stream"}]},{"cell_type":"code","source":"#pl epoch run\n#validation run\n#val accuracy print\n#test images plotting\n#sweep run\n#partB","metadata":{"id":"rtPOrNhIa_vA","outputId":"1df94eb8-e619-4c0a-e5ef-8edaaeb4fbfe","execution":{"iopub.status.busy":"2024-04-05T23:54:58.663335Z","iopub.execute_input":"2024-04-05T23:54:58.663794Z","iopub.status.idle":"2024-04-05T23:54:58.668823Z","shell.execute_reply.started":"2024-04-05T23:54:58.663755Z","shell.execute_reply":"2024-04-05T23:54:58.667375Z"},"editable":false,"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ps-HpKH0WSmy","editable":false},"execution_count":null,"outputs":[]}]}